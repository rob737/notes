---------------------------- Doubts ------------------------------------
1. Is there any way to just have atomic operations without using transaction ? (I think that's what optimistic locking is e.g. AtomicInteger etc)?

2. What is the role of Storage Engine?

3. What exactly is a secondary index?

4. When we have a transaction then why do we need different isolation levels?
-- While transactions help maintain atomicity, consistency, isolation, and durability (ACID), 
   the "isolation" part comes in varying degrees to control 
   how much one transaction is affected by others that are happening concurrently.

5. What is the criteria to create a snapshot of object in MVCC or Snapshot isolation ?
    Also, does it create snapshot of whole DB or only associated rows ?

6. How atomic operations prevent lost update anamoly ?

Answer :

An atomic operation guarantees that no other operation can observe the system in an intermediate state and no interleaving can happen during its execution.

In the above example, if incrementing x is done using an atomic increment operation like x++ (atomically):
Only one thread/process can read, increment, and write x at a time.
Others will have to wait or retry, ensuring no updates are lost.

Mechanisms for Atomicity:

 - Low-level (in-memory)
 - CPU instructions like LOCK XADD on x86
 - Java's AtomicInteger.incrementAndGet()
 - Compare-And-Swap (CAS) loops


7. Relation between CPU cycle and instruction execution ?

Answer :

 In ideal pipelined RISC CPUs, each stage (Fetch, Decode, Execute, etc.) is designed to take 1 CPU cycle.
 ❌ But actual instruction execution time (in cycles) may vary due to instruction type and CPU architecture.

 Pipelining improves throughput, not the cycle time per instruction individually.
 But in Real CPUs (especially CISC like x86):
 Not every stage always takes 1 cycle.

 Complex instructions (e.g., DIV, MUL, memory access) may take multiple cycles even in a pipelined architecture.
 Modern CPUs have super-scalar, out-of-order, and speculative execution, making actual cycle counts much harder to predict.
 Cache misses, branch mispredictions, and instruction stalls can also increase cycle counts.


--------------------------- XXXXX ---------------------------------------------------


A transaction is a way for an application to group several reads and writes
together into a logical unit. Conceptually, all the reads and writes in a transaction are
executed as one operation: either the entire transaction succeeds (commit) or it fails
(abort, rollback).



-- The Slippery Concept of a Transaction

Almost all relational databases today, and some nonrelational databases, support transactions.


-- Atomicity :

In multi-threaded programming, if one thread executes an atomic
operation, that means there is no way that another thread could see the half-finished
result of the operation. 

In a nutshell, atomicity is basically that the system can only be in the state it was before the operation or after the operation, not something in between.

-- Consistency :

Consistent hashing is an approach to partitioning that some systems use for rebalancing.

In the context of ACID, consistency refers to an application-specific notion of the database being in a “good state”.
i.e. it should be in a correct state wrt to the use case like debit and credit reconciliation.


Atomicity, isolation, and durability are properties of the database, whereas consistency (in the ACID sense) is a property of the application. 
The application may rely on the database’s atomicity and isolation properties in order to achieve consistency,
but it’s not up to the database alone.

-- Isolation :

The classic database
textbooks formalize isolation as serializability, which means that each transaction can
pretend that it is the only transaction running on the entire database. The database
ensures that when the transactions have committed, the result is the same as if they
had run serially (one after another), even though in reality they may have run concurrently.

However, in practice, serializable isolation is rarely used, because it carries a performance penalty.


-- Durability :

Durability is the promise that once a transaction has committed successfully, any data it has written will not be forgotten, even if there is a
hardware fault or the database crashes.

In a single-node database, durability typically means that the data has been written to nonvolatile storage such as a hard drive or SSD. 
It usually also involves a write-ahead log.

In a replicated database, durability may mean that the data has been successfully copied to some number of nodes.

-- Single-Object and Multi-Object Operations :

Single Object basically denotes same table and Multi-Object denotes multiple tables in context of relational database.


Dirty read :
One transaction reads another transaction's uncommitted writes.


For Single Object :

Atomicity can be implemented using a log for crash recovery
(see “Making B-trees reliable” on page 82), and isolation can be implemented
using a lock on each object (allowing only one thread to access an object at any one
time).

Similarly popular is a compare-and-set operation, which allows a write to
happen only if the value has not been concurrently changed by someone else (see
“Compare-and-set” on page 245).

Compare-and-set and other single-object operations have been dubbed “lightweight
transactions” or even “ACID” for marketing purposes, but that
terminology is misleading. A transaction is usually understood as a mechanism for
grouping multiple operations on multiple objects into one unit of execution.

-- Handling errors and aborts :

If you want to make sure that several different systems either commit or
abort together, two-phase commit can help (we will discuss this in “Atomic
Commit and Two-Phase Commit (2PC)” on page 354)


-- Weak Isolation Levels :

Concurrency issues (race conditions) only
come into play when one transaction reads data that is concurrently modified by
another transaction, or when two transactions try to simultaneously modify the same
data.

In practice, isolation is unfortunately not that simple. Serializable isolation has a performance
cost, and many databases don’t want to pay that price. It’s therefore
common for systems to use weaker levels of isolation, which protect against some
concurrency issues, but not all. Those levels of isolation are much harder to understand,
and they can lead to subtle bugs, but they are nevertheless used in practice.

"Rather than blindly relying on tools, we need to develop a good understanding of the
kinds of concurrency problems that exist, and how to prevent them."

In this section, we will look at several weak (nonserializable) isolation levels.


-- Read Committed :

The most basic level of transaction isolation is read committed. 

It makes two guarantees:

1. When reading from the database, you will only see data that has been committed.
(no dirty reads).
2. When writing to the database, you will only overwrite data that has been committed
(no dirty writes).

What exactly is dirty read ?

Imagine a transaction has written some data to the database, but the transaction has
not yet committed or aborted. Can another transaction see that uncommitted data? If
yes, that is called a dirty read.

Transactions running at the read committed isolation level must prevent dirty reads.
This means that any writes by a transaction only become visible to others when that
transaction commits (and then all of its writes become visible at once).


-- No dirty writes :

However, what happens if the earlier write is part of a transaction that has not yet
committed, so the later write overwrites an uncommitted value? This is called a dirty
write.

Read committed solves both Dirty read and Dirty write.


-- Implementing read committed :

Most commonly, databases prevent dirty writes by using row-level locks: when a
transaction wants to modify a particular object (row or document), it must first
acquire a lock on that object. It must then hold that lock until the transaction is committed
or aborted. Only one transaction can hold the lock for any given object; if
another transaction wants to write to the same object, it must wait until the first
transaction is committed or aborted before it can acquire the lock and continue. This
locking is done automatically by databases in read committed mode (or stronger isolation
levels).

In short, read committed isolation level is implemented using row level locking.

Most databases prevent dirty reads using the approach illustrated in
Figure 7-4 i.e. for every object that is written, the database remembers both the old committed
value and the new value set by the transaction that currently holds the write
lock. While the transaction is ongoing, any other transactions that read the object are
simply given the old value. Only when the new value is committed, do transactions
switch over to reading the new value.


-- Snapshot Isolation and Repeatable Read Isolation levels :

Non Repeatable read or read skew : In this anamoly with read committed isolation level, there can be instances where result of a query when read was perfomed 
while transaction is in progress may be different once it is performed after the transaction is committed.
e.g. Let's say you are calculating a sum of a column, while you are reading the value then at the same time some transaction is initiated which modifies
the value but you will not see it with read committed isolation level so your value of sum will be different if you re-calculate it after the transaction is completed.


some situations cannot tolerate such temporary inconsistency: 

Backups :
Taking a backup requires making a copy of the entire database, which may take
hours on a large database. During the time that the backup process is running,
writes will continue to be made to the database. Thus, you could end up with
some parts of the backup containing an older version of the data, and other parts
containing a newer version. If you need to restore from such a backup, the
inconsistencies (such as disappearing money) become permanent.

Analytic queries and integrity checks
Sometimes, you may want to run a query that scans over large parts of the database.
Such queries are common in analytics (see “Transaction Processing or Analytics?”
on page 90), or may be part of a periodic integrity check that everything
is in order (monitoring for data corruption). These queries are likely to return
nonsensical results if they observe parts of the database at different points in
time.

To mitigate Non Repeatable read or read skew anamoly, Snapshot isolation is the most common solution.

-- Snapshot isolation

The idea is that
each transaction reads from a consistent snapshot of the database—that is, the transaction
sees all the data that was committed in the database at the start of the transaction.
Even if the data is subsequently changed by another transaction, each
transaction sees only the old data from that particular point in time.

Snapshot isolation is a popular feature: it is supported by PostgreSQL, MySQL with
the InnoDB storage engine, Oracle etc.

-- Implementing snapshot isolation

Like read committed isolation, implementations of snapshot isolation typically use
write locks to prevent dirty writes (see “Implementing read committed” on page 236),
which means that a transaction that makes a write can block the progress of another
transaction that writes to the same object. However, reads do not require any locks.
From a performance point of view, a key principle of snapshot isolation is readers
never block writers, and writers never block readers.

To implement snapshot isolation, databases use a generalization of the mechanism
we saw for preventing dirty reads in Figure 7-4. The database must potentially keep
several different committed versions of an object, because various in-progress transactions
may need to see the state of the database at different points in time. Because it
maintains several versions of an object side by side, this technique is known as multiversion
concurrency control (MVCC).

Storage engines that support snapshot isolation typically use MVCC for their read committed isolation level as well.


How snapshot isolation is implemented in PostgreSQL :

When a transaction is started, it is
given a unique, always-increasingvii transaction ID (txid). Whenever a transaction
writes anything to the database, the data it writes is tagged with the transaction ID of
the writer.

Each row in a table has a created_by field, containing the ID of the transaction that
inserted this row into the table. Moreover, each row has a deleted_by field, which is
initially empty. If a transaction deletes a row, the row isn’t actually deleted from the
database, but it is marked for deletion by setting the deleted_by field to the ID of the
transaction that requested the deletion. At some later time, when it is certain that no
transaction can any longer access the deleted data, a garbage collection process in the
database removes any rows marked for deletion and frees their space.

An update is internally translated into a delete and a create. For example, in
Figure 7-7, transaction 13 deducts $100 from account 2, changing the balance from
$500 to $400. The accounts table now actually contains two rows for account 2: a
row with a balance of $500 which was marked as deleted by transaction 13, and a row
with a balance of $400 which was created by transaction 13.

Visibility rules for Snapshot isolation :

Basically snapshot is taken at the point of need with already committed data so transactions that are in progress/aborted or 
started after taking the snapshot is ignored.

Using visibility rules, the database can present a consistent snapshot of the database to the application. This
works as follows:

1. At the start of each transaction, the database makes a list of all the other transactions
that are in progress (not yet committed or aborted) at that time. Any writes
that those transactions have made are ignored, even if the transactions subsequently
commit.
2. Any writes made by aborted transactions are ignored.
3. Any writes made by transactions with a later transaction ID (i.e., which started
after the current transaction started) are ignored, regardless of whether those
transactions have committed.
4. All other writes are visible to the application’s queries. 

By never updating values in place but instead creating a new
version every time a value is changed, the database can provide a consistent snapshot while incurring only a small overhead.

-- Indexes and Snapshot isolation :

How do indexes work in a multi-version database? One option is to have the index
simply point to all versions of an object and require an index query to filter out any
object versions that are not visible to the current transaction. When garbage collection
removes old object versions that are no longer visible to any transaction, the corresponding
index entries can also be removed.

In practice, many implementation details determine the performance of multiversion
concurrency control. For example, PostgreSQL has optimizations for avoiding
index updates if different versions of the same object can fit on the same page.

-- Repeatable read and naming confusion

Snapshot isolation is a useful isolation level, especially for read-only transactions.
However, many databases that implement it call it by different names. In Oracle it is
called serializable, and in PostgreSQL and MySQL it is called repeatable read.

-- Preventing lost updates :

The read committed and snapshot isolation levels we’ve discussed so far have been
primarily about the guarantees of what a read-only transaction can see in the presence
of concurrent writes.

The lost update problem can occur if an application reads some value from the database,
modifies it, and writes back the modified value (a read-modify-write cycle). If
two transactions do this concurrently, one of the modifications can be lost, because
the second write does not include the first modification. (We sometimes say that the
later write clobbers the earlier write.)

Because this is such a common problem, a variety of solutions have been developed as follows :

-- Atomic write operations :

Many databases provide atomic update operations, which remove the need to implement
read-modify-write cycles in application code. They are usually the best solution
if your code can be expressed in terms of those operations. For example, the following
instruction is concurrency-safe in most relational databases:
UPDATE counters SET value = value + 1 WHERE key = 'foo';

Atomic operations are usually implemented by taking an exclusive lock on the object
when it is read so that no other transaction can read it until the update has been applied.

Note :
Unfortunately, object-relational mapping frameworks make it easy to accidentally
write code that performs unsafe read-modify-write cycles instead of using atomic
operations provided by the database. That’s not a problem if you know what you
are doing, but it is potentially a source of subtle bugs that are difficult to find by
testing.


-- Explicit locking :

Another option for preventing lost updates, if the database’s built-in atomic operations
don’t provide the necessary functionality, is for the application to explicitly lock
objects that are going to be updated. Then the application can perform a readmodify-
write cycle, and if any other transaction tries to concurrently read the same
object, it is forced to wait until the first read-modify-write cycle has completed.

Example :

BEGIN TRANSACTION;
SELECT * FROM figures
WHERE name = 'robot' AND game_id = 222
FOR UPDATE;
-- Check whether move is valid, then update the position
-- of the piece that was returned by the previous SELECT.
UPDATE figures SET position = 'c4' WHERE id = 1234;
COMMIT;


The FOR UPDATE clause indicates that the database should take a lock on all rows
returned by this query.

Indeed, PostgreSQL’s repeatable read, Oracle’s
serializable, and SQL Server’s snapshot isolation levels automatically detect when a
lost update has occurred and abort the offending transaction.

-- Compare & Set :

In databases that don’t provide transactions, you sometimes find an atomic compareand-
set operation.

However, if the database allows the WHERE clause to read from an old snapshot,
this statement may not prevent lost updates, because the condition may be true even
though another concurrent write is occurring. Check whether your database’s
compare-and-set operation is safe before relying on it.

-- Conflict resolution and replication :

Locks and compare-and-set operations assume that there is a single up-to-date copy
of the data. However, databases with multi-leader or leaderless replication usually
allow several writes to happen concurrently and replicate them asynchronously, so
they cannot guarantee that there is a single up-to-date copy of the data. Thus, techniques
based on locks or compare-and-set do not apply in this context. (We will revisit
this issue in more detail in “Linearizability” on page 324.)

Atomic operations can work well in a replicated context, especially if they are commutative
(i.e., you can apply them in a different order on different replicas, and still
get the same result). For example, incrementing a counter or adding an element to a
set are commutative operations. That is the idea behind Riak 2.0 datatypes, which
prevent lost updates across replicas. When a value is concurrently updated by different
clients, Riak automatically merges together the updates in such a way that no
updates are lost.


-- Write Skew & Phantoms :

Note : Different objects may mean different rows of a table as well.

You can think of write skew as a generalization of the lost update problem. Write
skew can occur if two transactions read the same objects, and then update some of
those objects (different transactions may update different objects). In the special case
where different transactions update the same object, you get a dirty write or lost
update anomaly (depending on the timing).

So write skew is basically there is a precondition based on which decision is made whether to write 
a particular output or not.
Initially, precondition is met but the time write is supposed to happen, precondition is changed by another transaction altogether.
e.g. slot booking system where condition can be a particular slot should be free between a partiucular time.

This effect, where a write in one transaction changes the result of a search query in
another transaction, is called a phantom [3]. Snapshot isolation avoids phantoms in
read-only queries, but in read-write transactions like the examples we discussed,
phantoms can lead to particularly tricky cases of write skew.

-- Serializability :

Serializable isolation is usually regarded as the strongest isolation level. It guarantees
that even though transactions may execute in parallel, the end result is the same as if
they had executed one at a time, serially, without any concurrency. Thus, the database
guarantees that if the transactions behave correctly when run individually, they continue
to be correct when run concurrently—in other words, the database prevents all
possible race conditions.

Most databases that provide serializability today use one of three techniques, which we will explore in the
rest of this chapter:
• Literally executing transactions in a serial order (see “Actual Serial Execution” on
page 252)
• Two-phase locking (see “Two-Phase Locking (2PL)” on page 257), which for several
decades was the only viable option
• Optimistic concurrency control techniques such as serializable snapshot isolation
(see “Serializable Snapshot Isolation (SSI)” on page 261)

-- Actual Serial Execution :

The approach of executing transactions serially is implemented in VoltDB/H-Store, Redis, and Datomic.

One of the problem with stored procedure :

A database is often much more performance-sensitive than an application server,
because a single database instance is often shared by many application servers. A
badly written stored procedure (e.g., using a lot of memory or CPU time) in a
database can cause much more trouble than equivalent badly written code in an
application server.

Modern implementations of stored procedures
have abandoned PL/SQL and use existing general-purpose programming languages
instead: VoltDB uses Java or Groovy, Datomic uses Java or Clojure, and Redis
uses Lua.

Note : VoltDB also uses stored procedures for replication: instead of copying a transaction’s
writes from one node to another, it executes the same stored procedure on each replica.
VoltDB therefore requires that stored procedures are deterministic (when run on
different nodes, they must produce the same result).

-- 2PL (Two Phase Locking) :

For around 30 years, there was only one widely used algorithm for serializability in
databases: two-phase locking (2PL).

Two-phase locking is similar, but makes the lock requirements much stronger. Several
transactions are allowed to concurrently read the same object as long as nobody
is writing to it. 

But as soon as anyone wants to write (modify or delete) an object,
exclusive access is required:
• If transaction A has read an object and transaction B wants to write to that
object, B must wait until A commits or aborts before it can continue. (This
ensures that B can’t change the object unexpectedly behind A’s back.)
• If transaction A has written an object and transaction B wants to read that object,
B must wait until A commits or aborts before it can continue. (Reading an old
version of the object, like in Figure 7-1, is not acceptable under 2PL.)

2PL is used by the serializable isolation level in MySQL (InnoDB).

-- Implementation of 2 Phase Locking :

The blocking of readers and writers is implemented by a having a lock on each object
in the database. The lock can either be in shared mode or in exclusive mode. The lock
is used as follows:

• If a transaction wants to read an object, it must first acquire the lock in shared
mode. Several transactions are allowed to hold the lock in shared mode simultaneously,
but if another transaction already has an exclusive lock on the object,
these transactions must wait.

• If a transaction wants to write to an object, it must first acquire the lock in exclusive
mode. No other transaction may hold the lock at the same time (either in
shared or in exclusive mode), so if there is any existing lock on the object, the
transaction must wait.

• If a transaction first reads and then writes an object, it may upgrade its shared
lock to an exclusive lock. The upgrade works the same as getting an exclusive
lock directly.

• After a transaction has acquired the lock, it must continue to hold the lock until
the end of the transaction (commit or abort). This is where the name “twophase”
comes from: the first phase (while the transaction is executing) is when
the locks are acquired, and the second phase (at the end of the transaction) is
when all the locks are released.

Since so many locks are in use, it can happen quite easily that transaction A is stuck
waiting for transaction B to release its lock, and vice versa. This situation is called
deadlock. The database automatically detects deadlocks between transactions and
aborts one of them so that the others can make progress. The aborted transaction
needs to be retried by the application.


Two phase locking is rarely used because of it's performance bottleneck.

-- Predicate locks :

It works similarly to the shared/exclusive lock described earlier, but rather than belonging to a
particular object (e.g., one row in a table), it belongs to all objects that match some
search condition, such as:

SELECT * FROM bookings
WHERE room_id = 123 AND
end_time > '2018-01-01 12:00' AND
start_time < '2018-01-01 13:00';

A predicate lock restricts access as follows:

• If transaction A wants to read objects matching some condition, like in that
SELECT query, it must acquire a shared-mode predicate lock on the conditions of
the query. If another transaction B currently has an exclusive lock on any object
matching those conditions, A must wait until B releases its lock before it is
allowed to make its query.

• If transaction A wants to insert, update, or delete any object, it must first check
whether either the old or the new value matches any existing predicate lock. If
there is a matching predicate lock held by transaction B, then A must wait until B
has committed or aborted before it can continue.

-- Index range locks : (260)

Unfortunately, predicate locks do not perform well: if there are many locks by active
transactions, checking for matching locks becomes time-consuming. For that reason,
most databases with 2PL actually implement index-range locking (also known as nextkey
locking), which is a simplified approximation of predicate locking.

-- Serializable Snapshot Isolation :

Today SSI is used both in single-node databases (the serializable isolation level in
PostgreSQL since version 9.1 [41]) and distributed databases (FoundationDB uses a
similar algorithm). As SSI is so young compared to other concurrency control mechanisms,
it is still proving its performance in practice.


-- Pessimistic versus optimistic concurrency control :

Pessimistic concurrency control : basically 2 PL and other locking mechanisms to safeguard against future contention issues.

Optimistic concurrency control : let transactions do whatever they want to but before committing check if any integrity violations have happened.
If so then fail the transaction and retry.
e.g. using compareAndSet and then retry in case of condition not matching as needed.

Also, SSI is an example of optimistic locking.

-- Detecting stale MVCC reads

Recall that snapshot isolation is usually implemented by multi-version concurrency
control (MVCC; see Figure 7-10). When a transaction reads from a consistent snapshot
in an MVCC database, it ignores writes that were made by any other transactions
that hadn’t yet committed at the time when the snapshot was taken. In
Figure 7-10, transaction 43 sees Alice as having on_call = true, because transaction
42 (which modified Alice’s on-call status) is uncommitted. However, by the time
transaction 43 wants to commit, transaction 42 has already committed. This means
that the write that was ignored when reading from the consistent snapshot has now
taken effect, and transaction 43’s premise is no longer true.

In order to prevent this anomaly, the database needs to track when a transaction
ignores another transaction’s writes due to MVCC visibility rules. When the transaction
wants to commit, the database checks whether any of the ignored writes have
now been committed. If so, the transaction must be aborted.


Note: There is a transaction manager which identify whether read has become stale or not.

Note: each machine on the network has its own clock, which is an actual hardware
device: usually a quartz crystal oscillator. These devices are not perfectly accurate,
so each machine has its own notion of time, which may be slightly faster.




