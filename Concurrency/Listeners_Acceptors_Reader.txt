------------------------------------- Doubt -------------------------------------
1. When we start a server and deploy our backend bundle to it then effectively are we starting two processes?
   one for server where conenction request is listened on and other is for backend instruction execution? 	 
   e.g. in node, we were creating clusters with multiple process which reads up the same queues for faster execution.
   
2. What is the difference between a connection and a request ?   

3. What actually is the significance of a port ?


------------------------ XXXXX -------------------------------


-- Listener :
 Thread or process that calls listen and pass in the port and address.
 basically, this process listens to client requests.
 
-- Acceptor :
 Thread or process that calls accept method on a particular socket filedescriptor to get client connection/filedescriptor.

-- Reader :
 Thread or process that calls read to read from read buffer after connection is created by picking entry from accept queue.
 
Note : There can be multiple ways to design it, either a single process or thread can do the job of all 3 (listener, acceptor, reader/writer)
	   or we can distribute it among multithreads or multiprocess.	
 

---- Single Listener / Single Worker Thread :

We have a single process that listens, accepts and read connections.

Node follows the same principle and so in case of many connections single thread of a process may lead to congestion in accept queue.

One optimization that node does is clustering i.e. basically it spins up a new process that listens/accepts/reads to the same socket queues.
Thus, effectively increasing the throughput. 


---- Single Listener / Multiple Worker Threads :

By Worker, we mean Reader.

Note : MemCacheD has this architecture.

Single process and multiple threads.

That process is the listener and the acceptor.

Multiple threads are the readers.

Process has the socket and is listening for the connections, once kernel puts the connection in the accept queue then this process
can pick it up and puts it it's memory space and then spawn threads to complete the execution as fast as possible.
or process can assign each thread a connection.

Note : These threads are dealing directly with connection, there can be other threads that deals with processing the requests.

Note: The issue with this design is that one thread can be over consumed all the time doing all the hefty things and other thread is free most of the time.

---- Single Listener / Multiple worker threads but with load balancing :

Single process itself listens and accepts connection like previous.
However, instead of passing connection itself to the readers, same process reads the connection and passes on the request to multiple threads for processing
business logic.

Note : RAMCloud has this architecture.

Note : Socket Sharding is the technique using which multiple processes can listen to the same socket. Now, my doubt is clear about how 
a cluster with multiple processes work.

We can have multiple processes doing the listening, accepting and reading connections and then threads can do all the business processing tasks.


---- Multiple Acceptor threads on a Single Socket :

Note: Nginx leverages this concept.

So, basically the concept is that single process would listen to the socket.

Now, multiple threads guarded by mutex as socket is critical section would accept and read the connections and then process it.

Technically, each thread owns the connection.


Informally, flow would be like this :

A process is identified by a port so there are technically a separate kernel process and a process for our server.

Now, after kernel is done with 3 way handhsake with client, it puts it into socket's accept queue and somehow inform server process that it has done it.

or may be socket process does polling which is effectively called listening.

Now, if it finds a connection then it instructs a thread to accept it and read it to perform business logic.


---- Multiple Listener process on the same port :

Concept of socket sharding is what allows multiple process to listen to on the same port in the same machine/IP.

While creating socket, we can leverage SO_REUSEPORT to enable sharing socket with multiple processes.

OS creates multiple queues (SYN,ACCEPT,READ,WRITE) for each process and does load balancing to allocate connections to each process.

Note: This is almost the default in all proxies. (Nginx, HAProxy, envoy etc).


---- Backend Idempotency : 

How do I avoid executing the same request twice.

Idempotency is basically resending the same request without affecting the backend.

what do you mean by "without affecting the backend" ?

Let's imagine you are doing payment and due to some circumstances, your payment request has reached backend and is processed but somehow 
client crashed and didn't register that it was successfully completed.

Client may give you an option to retry the payment, in which case it may lead to duplicate payments which is troublesome.


Example of backend API which is not idempotent :

let's say we do POST /comment so everytime this API is called it would insert a new row with the comment.
However, there is a possibility like mentioned above that client may retry under some circumstances which may lead to duplicate entries being 
stored using comment. 

This is an important factor to consider as sometimes layer 7 proxies does retry which is totally out of control.

upsert can help in these cases.

Note: Browser and proxies treat GET as idempotent and retry by default so never use GET request to update something.

CDNs are basically reverse proxies.